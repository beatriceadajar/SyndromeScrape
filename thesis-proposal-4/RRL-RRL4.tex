\section{Retrieving information from websites}

The web is littered with unindexed and freeform text that are usually not machine-readable, causing these to be ``inaccessible to automated knowledge discovery techniques'' \cite{soderland1997learning}. Despite this, crucial information can still be extracted from websites. Within these unstructured texts are systematic information such as names, dates, and figures that are desirable for a variety of purposes \cite{munzert2014automated}.

\subsection{Web Scraping}
A process that can retrieve such information is called web scraping, and involves fetching web pages then extracting data from them through ``parsing [web pages] and extracting out data points relative to their structure'' \cite{penman2009web}. Web scraping converts this unstructured data (usually in the form of HTML or XML pages), into information that is stored in databases for analysis \cite{vargiu2012exploiting}. Web scraping involves the following processes \cite{munzert2014automated}:

\begin{enumerate}
\item Gathering unstructured data; 
\item Determining noticeable patterns behind the desired information;  and
\item Apply such patterns to unstructured data to extract information
\end{enumerate}

One technique that can be used to scrape data from websites uses automatically generated wrappers for parse HTML files to extract the desired information \cite{crescenzi2001roadrunner}. The technique follows \textit{ACME}:
\begin{enumerate}
\item \textit{Align} HTML code from two HTML pages;
\item Iteratively \textit{Collapse} under \textit{Mismatch}, to find mismatches specified by the wrapper to find common regular expressions between the two pages; and
\item \textit{Extract} data by parsing the text that remains.
\end{enumerate}

\subsection{Data Classification and Topic Modeling}
Before this extracted data can be useful, it should be refined further into data relevant to the purposes through classification and topic modeling techniques.

The problem of data classification, according to Aggarwal, can be stated as follows: ``Given a set of training data points along with associated training labels, determine the class label for an unlabeled test instance'' \cite{aggarwal2014data}. With such a problem, there are normally two phases. The first one, the training phase, deals with constructing a model from a given training dataset. The second one, the testing phase, deals with using the constructed model to assign labels to unlabeled test data points. The end result should be that all unlabeled test instances are correctly labeled by the classifier \cite{aggarwal2014data}.

Probabilistic topic modeling is a set of algorithms designed to allocate documents of text to specific themes or topics. They analyze the text, draw connections between words and themes, and use that to group documents under the generated themes. These algorithms can be adapted to different kinds of data, whether structured or unstructured \cite{blei2012probabilistic}.

With that, data classification and topic modeling techniques could be used to determine correlations between symptoms with ailments \cite{paul2012model} found in web scraped articles that could be potentially useful in syndromic surveillance. 